{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "def unpickle(file_path):\n",
    "\t\"\"\"\n",
    "\tUnpickles a cifar batch file.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tfile_path : string\n",
    "\t\tpath to the cifar10 batch file, which is a Python \"pickled\" object produced with cPickle. \n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdict\n",
    "\t\ta dictioniary with the following elements:\n",
    "\t\t\n",
    "\t\tdata (string) -> numpy.ndarray\n",
    "\t\t\ta 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "\t\t\tThe first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "\t\t\tThe image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "\t\t\tof the first row of the image.\n",
    "\t\t\t\n",
    "\t\tlabels (string) -> list\n",
    "\t\t\ta list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image \n",
    "\t\t\tin the array data.\n",
    "\t\"\"\"\n",
    "\twith open(file_path, 'rb') as f:\n",
    "\t\tdatadict = cPickle.load(f)\n",
    "\treturn datadict\n",
    "\n",
    "def load_cifar_batches(root_path):\n",
    "\t\"\"\"\n",
    "\tLoad all cifar batches into a dictionary.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\troot_path\n",
    "\t\tpath to directory where all 6 cifar10 batch files are stored\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdict\n",
    "\t\ta dictionary with the following elements: \n",
    "\n",
    "\t\ttraining_data (string) -> numpy.ndarray\n",
    "\t\t\ta 50000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image.\t\t\t\n",
    "\n",
    "\t\ttraining_labels (string) -> numpy.ndarray\n",
    "\t\t\ta numpy array of 50000 vectors that each represent labels from 0 to 9. The number at index i indicates the label \n",
    "\t\t\tof the ith image in the training data.\n",
    "\n",
    "\t\ttest_data (string) -> numpy.ndarray\n",
    "\t\t\ta 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image.\t\t\n",
    "\t\t\n",
    "\t\ttest_labels (string) -> numpy.ndarray\n",
    "\t\t\ta numpy array of 50000 vectors that each represent labels from 0 to 9. The number at index i indicates the label \n",
    "\t\t\tof the ith image in the test data.\n",
    " \t\"\"\"\n",
    " \tdataset = {}\n",
    "\n",
    " \t# training data (5 batch files)\n",
    " \ttraining_data = []\n",
    " \ttraining_labels = []\n",
    " \tfor i in range(1, 2): # range(1, 6) for all 5 datasets, use just 1 for development\n",
    " \t\tbatch_file_name = 'data_batch_' + str(i)\n",
    " \t\tfile_path = os.path.join(root_path, batch_file_name)\n",
    " \t\t\n",
    " \t\tdatadict = unpickle(file_path)\n",
    " \t\n",
    " \t\ttraining_data.extend(datadict['data'])\n",
    " \t\ttraining_labels.extend(datadict['labels'])\n",
    "\n",
    " \tdataset['training_data'] = np.array(training_data)\n",
    " \tdataset['training_labels'] = np.array([vectorize_label(label) for label in training_labels])\n",
    "\n",
    " \t# test data (1 batch file)\n",
    " \tfile_path = os.path.join(root_path, 'test_batch')\n",
    "\n",
    " \tdatadict = unpickle(file_path)\n",
    " \tdataset['test_data'] = datadict['data']\n",
    " \tdataset['test_labels'] = np.array(datadict['labels'])\n",
    "\n",
    " \treturn dataset\n",
    "\n",
    "@np.vectorize\n",
    "def vectorize_label(n):\n",
    "\t\"\"\"\n",
    "\tConstructs a 10 x 1 vector of length 10 based on the input, so that the (n-1)-th element in the vector is 1 and all the other\n",
    "\telements are 0.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tint : label\n",
    "\t\tThe label of the class.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnumpy.ndarray\n",
    "\t\tThe vectorized representation of the label.\t\n",
    "\t\"\"\"\n",
    "\tv = np.zeros(10)\n",
    "\tv[n] = 1\n",
    "\treturn v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "len(x)3072\n",
      "len(x[0])10\n",
      "len(w1)1541\n",
      "len(w1[0])3072\n",
      "len(w2)10\n",
      "len(w2[0])1541\n",
      "len(b1)1541\n",
      "len(b1[0])1\n",
      "len(b2)10\n",
      "len(b2[0])1\n",
      "len(dy_da)1541\n",
      "len(dy_da[0])10\n",
      "len(y)1541\n",
      "len(y[0])10\n",
      "len(dloss_dz)10\n",
      "len(dloss_dz[0])10\n",
      "len(dloss_dy)1541\n",
      "len(dloss_dy[0])10\n",
      "len(dloss_da)1541\n",
      "len(dloss_da[0])10\n",
      "dloss_dw1[[ 0.14044871  0.1365376   0.11570444 ...,  0.15015971  0.14013992\n",
      "   0.13222632]\n",
      " [ 0.07311027  0.05873417  0.05760976 ...,  0.00784817  0.01347021\n",
      "   0.01930434]\n",
      " [-0.17713218 -0.16077266 -0.15222394 ..., -0.07362224 -0.06708658\n",
      "  -0.07012962]\n",
      " ..., \n",
      " [ 0.01925181  0.03692618  0.03006364 ..., -0.00236142  0.00096526\n",
      "  -0.00323383]\n",
      " [ 0.00504042  0.02233106  0.02280095 ...,  0.03314592  0.03149648\n",
      "   0.02741346]\n",
      " [-0.12736011 -0.11270126 -0.11245256 ..., -0.06905135 -0.05598265\n",
      "  -0.05758536]]\n",
      "dloss_dw2[[  8.56811872e-01   1.05774948e+00   3.29961900e-01 ...,   4.34137891e-01\n",
      "    3.33603910e-01   9.51719821e-04]\n",
      " [ -1.15615499e+00  -1.01602779e+00  -5.29378260e-01 ...,   3.41417340e-01\n",
      "   -6.32281253e-01   6.15469479e-04]\n",
      " [ -1.79560854e-01   1.98781855e-03   2.39138137e-01 ...,  -3.21587202e-01\n",
      "    3.08714671e-01   9.21640435e-04]\n",
      " ..., \n",
      " [ -1.96455613e-01  -8.95631813e-03  -1.43124531e-01 ...,  -4.18381991e-01\n",
      "    2.95607695e-01   9.00868781e-04]\n",
      " [ -1.84734865e-01   8.81271878e-03  -4.26055222e-01 ...,  -5.31011114e-01\n",
      "    3.14604121e-01   9.35206428e-04]\n",
      " [ -2.69540379e-01  -1.05810483e+00  -8.93155871e-02 ...,  -4.71378845e-01\n",
      "   -1.68001922e-01   8.26663920e-04]]\n",
      "len(dloss_dw1)1541\n",
      "len(dloss_dw2)10\n",
      "len(dloss_dw1[0])3072\n",
      "len(dloss_dw2[0])1541\n",
      "np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1)[[ 0.00061556]\n",
      " [ 0.00022338]\n",
      " [-0.0008264 ]\n",
      " ..., \n",
      " [-0.00028902]\n",
      " [ 0.00017862]\n",
      " [-0.00068668]]\n",
      "np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[[ 1.06853012]\n",
      " [-1.00698119]\n",
      " [ 0.01199658]\n",
      " [-0.00403084]\n",
      " [-0.02035514]\n",
      " [ 0.99411021]\n",
      " [-0.00884548]\n",
      " [-0.00463646]\n",
      " [ 0.01902362]\n",
      " [-1.04881142]]\n",
      "len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))1541\n",
      "len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[0])10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-87f6f445e987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1541\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-87f6f445e987>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, training_labels, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    108\u001b[0m                                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdloss_da\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[0])\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdloss_dz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdloss_da\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[0])\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdloss_dz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                                 \u001b[0;31m# perform weight update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import load_cifar_batches\n",
    "\n",
    "class Network():\n",
    "\tdef __init__(self, sizes):\n",
    "\t\tself.num_layers = len(sizes)\n",
    "\t\tself.sizes = sizes\n",
    "\n",
    "\t\t# initialize random weights\n",
    "\t\tself.w1 = 0.001 * np.random.randn(sizes[1], sizes[0]) # hidden layer weights \n",
    "\t\tself.w2 = 0.001 * np.random.randn(sizes[2], sizes[1]) # output layer weights\n",
    "\n",
    "\t\t# initialize biases \n",
    "\t\tself.b1 = 0.001 * np.random.randn(sizes[1], 1)\n",
    "\t\tself.b2 = 0.001 * np.random.randn(sizes[2], 1)\n",
    "\n",
    "\tdef predict(self, test_data):\n",
    "\t\tpredictions = []\n",
    "\n",
    "\t\tfor x in test_data:\n",
    "\t\t\tx = x.reshape(3072, 1)\n",
    "\t\t\ty = Network.sigmoid(np.dot(self.w1, x) + self.b1)\n",
    "\t\t\tz = np.dot(self.w2, y) + self.b2\n",
    "\n",
    "\t\t\tpredictions.append(z.argmax())\n",
    "\n",
    "\t\treturn np.array(predictions)\n",
    "\n",
    "\tdef train(self, training_data, training_labels, batch_size=10, epochs=1, learning_rate=0.1):\n",
    "\t\tdata_mini_batches = [training_data[n:n+batch_size] for n in range(0, len(training_data), batch_size)]\n",
    "\t\tlabel_mini_batches = [training_labels[n:n+batch_size] for n in range(0, len(training_labels), batch_size)]\n",
    "\t\t\n",
    "\t\t# calculate batches in 1/10 epoch for plotting \n",
    "\t\tbatches_in_tenth_epoch = len(training_data) / 10 / batch_size\n",
    "\t\ttenth_epoch_losses = []\n",
    "\t\ttenth_epoch_loss = 0\n",
    "\n",
    "\t\ttotal_instances = len(training_data) * epochs\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tfor i, batch in enumerate(data_mini_batches):\n",
    "\t\t\t\t# print how many percentage till finished training \n",
    "\t\t\t\tpercentage = round(float(i * batch_size + epoch * len(training_data)) / total_instances * 100, 2) \n",
    "\t\t\t\tprint str(percentage) + \"%\" \n",
    "\n",
    "\t\t\t\t# FEEDFORWARD\n",
    "\t\t\t\t# reshape array in order to easily calculate dot product\n",
    "\t\t\t\tx = batch.T\n",
    "\t\t\t\ty = Network.sigmoid(np.dot(self.w1, x) + self.b1)\n",
    "\t\t\t\tz = np.dot(self.w2, y) + self.b2\n",
    "\t\t\t\tp = np.apply_along_axis(self.softmax, 0, z)\n",
    "\n",
    "\t\t\t\t# calculate loss\n",
    "\t\t\t\tloss = np.zeros(batch_size)\n",
    "\t\t\t\tfor j, (a, b) in enumerate(zip(p.T, label_mini_batches[i])):\n",
    "\t\t\t\t\tloss[j] = self.cross_entropy_loss(a, b)\n",
    "\n",
    "\t\t\t\tbatch_loss = sum(loss) / batch_size\n",
    "\t\t\t\ttenth_epoch_loss += batch_loss\n",
    "\n",
    "\t\t\t\tif not epoch and not i:\n",
    "\t\t\t\t\ttenth_epoch_losses.append(tenth_epoch_loss)\n",
    "\t\t\t\t\ttenth_epoch_loss = 0\n",
    "\t\t\t\telif not i % batches_in_tenth_epoch:\n",
    "\t\t\t\t\ttenth_epoch_losses.append(tenth_epoch_loss / batches_in_tenth_epoch)\n",
    "\t\t\t\t\ttenth_epoch_loss = 0\n",
    "\n",
    "\t\t\t\t# BACKPROPAGATION\n",
    "\t\t\t\t# output layer (z) gradient\n",
    "\t\t\t\tdloss_dz = p - label_mini_batches[i].T\n",
    "\t\t\t\tdloss_dw2 = np.dot(dloss_dz, y.T)\n",
    "\n",
    "\t\t\t\t# hidden layer (y) gradient\n",
    "\t\t\t\tdloss_dy = np.dot(self.w2.T, dloss_dz)\n",
    "\t\t\t\tdy_da = self.sigmoid_derivative(y)\n",
    "\t\t\t\tdloss_da = dloss_dy * dy_da\n",
    "\t\t\t\tdloss_dw1 = np.dot(dloss_da, x.T)\n",
    "\t\t\t\tprint \"len(x)\"+ str(len(x))\n",
    "\t\t\t\tprint \"len(x[0])\"+ str(len(x[0]))\n",
    "\t\t\t\tprint \"len(w1)\"+ str(len(self.w1))\n",
    "\t\t\t\tprint \"len(w1[0])\"+ str(len(self.w1[0]))\n",
    "\t\t\t\tprint \"len(w2)\"+ str(len(self.w2))\n",
    "\t\t\t\tprint \"len(w2[0])\"+ str(len(self.w2[0]))\n",
    "\t\t\t\tprint \"len(b1)\"+ str(len(self.b1))\n",
    "\t\t\t\tprint \"len(b1[0])\"+ str(len(self.b1[0]))\n",
    "\t\t\t\tprint \"len(b2)\"+ str(len(self.b2))\n",
    "\t\t\t\tprint \"len(b2[0])\"+ str(len(self.b2[0]))\n",
    "\t\t\t\tprint \"len(dy_da)\"+ str(len(dy_da))\n",
    "\t\t\t\tprint \"len(dy_da[0])\"+ str(len(dy_da[0]))\n",
    "\t\t\t\tprint \"len(y)\"+ str(len(y))\n",
    "\t\t\t\tprint \"len(y[0])\"+ str(len(y[0]))\n",
    "\t\t\t\tprint \"len(dloss_dz)\"+ str(len(dloss_dz))\n",
    "\t\t\t\tprint \"len(dloss_dz[0])\"+ str(len(dloss_dz[0]))\n",
    "\t\t\t\tprint \"len(dloss_dy)\"+ str(len(dloss_dy))\n",
    "\t\t\t\tprint \"len(dloss_dy[0])\"+ str(len(dloss_dy[0]))\n",
    "\t\t\t\tprint \"len(dloss_da)\"+ str(len(dloss_da))\n",
    "\t\t\t\tprint \"len(dloss_da[0])\"+ str(len(dloss_da[0]))\n",
    "\t\t\t\tprint \"dloss_dw1\"+ str(dloss_dw1)\n",
    "\t\t\t\tprint \"dloss_dw2\"+ str(dloss_dw2)\n",
    "\t\t\t\tprint \"len(dloss_dw1)\"+ str(len(dloss_dw1))\n",
    "\t\t\t\tprint \"len(dloss_dw2)\"+ str(len(dloss_dw2))\n",
    "\t\t\t\tprint \"len(dloss_dw1[0])\"+ str(len(dloss_dw1[0]))\n",
    "\t\t\t\tprint \"len(dloss_dw2[0])\"+ str(len(dloss_dw2[0]))\n",
    "\t\t\t\tprint \"np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1)\"+ str(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))\n",
    "\t\t\t\tprint \"np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)\"+ str(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1))\n",
    "\t\t\t\tprint \"len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))\"+ str(len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1)))\n",
    "\t\t\t\tprint \"len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[0])\"+ str(len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)))\n",
    "\t\t\t\tprint \"len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))\"+ str(len(np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1))[0])\n",
    "\t\t\t\tprint \"len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1)[0])\"+ str(len(np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1))[0])\n",
    "\t\t\t\t# perform weight update\n",
    "\t\t\t\tself.w1 -= dloss_dw1 * learning_rate\n",
    "\t\t\t\tself.w2 -= dloss_dw2 * learning_rate\n",
    "\n",
    "\t\t\t\t# perform bias update\n",
    "\t\t\t\tself.b1 -= np.sum(dloss_da, axis=1).reshape(self.sizes[1], 1) * learning_rate\n",
    "\t\t\t\tself.b2 -= np.sum(dloss_dz, axis=1).reshape(self.sizes[2], 1) * learning_rate\n",
    "\n",
    "\t\t# plot epoch losses \n",
    "\t\tplt.plot(range(len(tenth_epoch_losses)), tenth_epoch_losses)\n",
    "\t\tplt.xlabel(\"1/10 epoch\")\n",
    "\t\tplt.ylabel(\"Loss\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef cross_entropy_loss(self, probability_distribution, target_vector):\n",
    "\t\t\"\"\"\n",
    "\t\tCross entropy loss function.\n",
    "\t\t\"\"\"\n",
    "\t\treturn -np.sum((target_vector * np.log(probability_distribution)))\n",
    "\n",
    "\tdef softmax(self, z):\n",
    "\t\t\"\"\"\n",
    "\t\tTakes a vector of arbitrary real-valued scores and squashes it to a vector of values \n",
    "\t\tbetween zero and one that sum to one. This vector represents a probability distribution\n",
    "\t\tover mutually exclusive alternatives.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tz : numpy.ndarray\n",
    "\t\t\ta vector with real values that represent the scores of each class. \n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tnumpy.ndarray\n",
    "\t\t\t a vector of values between zero and one that sum to one.\n",
    "\t\t\"\"\"\n",
    "\t\tf = z - np.max(z)\n",
    "\t\treturn np.exp(f) / np.sum(np.exp(f))\n",
    "\n",
    "\t@staticmethod\n",
    "\t@np.vectorize\n",
    "\tdef sigmoid(x):\n",
    "\t\t\"\"\" \n",
    "\t\tA function that takes a real-valued number and \"squashes\" it into range between 0 and 1, so\n",
    "\t\tthat large negative numbers become 0 and large positive numbers become 1.  \n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tx : numpy.ndarray\n",
    "\t\t\tan array of real values, each of them is given as an argument to the sigmoid function.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tnumpy.ndarray\n",
    "\t\t\tan array of real values that are \"squashed\" into range between 0 and 1 by the function.\n",
    "\t\t\"\"\" \n",
    "\t\tif x >= 0:\n",
    "\t\t\tz = np.exp(-x)\n",
    "\t\t\treturn 1 / (1 + z)\n",
    "\t\telse:\n",
    "\t\t\t# if x is less than zero then z will be small, denominator can't be zero because it's 1+z\n",
    "\t\t\tz = np.exp(x)\n",
    "\t\t\treturn z / (1 + z)\n",
    "\n",
    "\tdef sigmoid_derivative(self, y):\n",
    "\t\treturn self.sigmoid(y) * (1 - self.sigmoid(y))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef test_accuracy(x, y):\n",
    "\t\tcount = 0\n",
    "\t\tfor i, prediction in enumerate(x):\n",
    "\t\t\tif (prediction == y[i]):\n",
    "\t\t\t\tcount += 1\n",
    "\n",
    "\t\treturn float(count) / x.shape[0]\n",
    "\n",
    "root_path = ''\n",
    "d = load_cifar_batches(root_path)\n",
    "\n",
    "nn = Network([3072, 1541, 10])\n",
    "nn.train(d['training_data'], d['training_labels'])\n",
    "\n",
    "a = nn.predict(d['test_data'])\n",
    "b = d['test_labels']\n",
    "print \"Test accuracy: \" + str(Network.test_accuracy(a, b))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.array([[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=np.array([[1,2,3,4,5,1,2,3,4,5],[1,2,3,4,5,1,2,3,4,5],[1,2,3,4,5,1,2,3,4,5],[1,2,3,4,5,1,2,3,4,5],[1,2,3,4,5,1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array([[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  9, 16, 25],\n",
       "       [ 1,  4,  9, 16, 25],\n",
       "       [ 1,  4,  9, 16, 25],\n",
       "       [ 1,  4,  9, 16, 25],\n",
       "       [ 1,  4,  9, 16, 25]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[1,1],[1,1],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=np.array([[1,2],[3,4],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 9, 16],\n",
       "       [16, 25]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
